TASK_TYPE: train_videolcm_t2v_entrance
use_fp16: True
guide_scale: 9.0
ENABLE: true
use_ema: false
num_workers: 6
frame_lens: [16, 16, 16, 16]
sample_fps: [4,  4,  4, 4]
resolution: [448, 256]
vit_resolution: [224, 224]
num_timesteps: 1000
zero_terminal_snr: True
vid_dataset: {
    'type': 'VideoDataset',
    'data_list': ['data/vid_list.txt', ],
    'data_dir_list': ['data/videos/', ],
    'vit_resolution': [224, 224],
    'resolution': [448, 256],
    'get_first_frame': True,
    'max_words': 1000,
}
img_dataset: {
    'type': 'ImageDataset',
    'data_list': ['data/img_list.txt', ],
    'data_dir_list': ['data/images', ],
    'vit_resolution': [224, 224],
    'resolution': [448, 256],
    'max_words': 1000
}
use_fp16: True
chunk_size: 2
decoder_bs: 2
max_frames: 16
target_fps: 16      # FPS Conditions, not encoding fps
scale: 8
batch_size: 1
use_zero_infer: True 
# For important input
round: 1
seed: 888
test_list_path: data/text_list_for_videolcm.txt
vldm_cfg: configs/t2v_train.yaml
positive_prompt: ', cinematic, High Contrast, highly detailed, Unreal Engine 5, no blur, 4k render'
# test_model: workspace/model_bk/model_scope_0267000.pth
test_model: models/videolcm_t2v_non_ema_544000.pth
# test_model: /mnt/user/video_generation/cache/videolcm_t2v_non_ema_435000.pth
num_inference_steps: 4
embedder: {
    'type': 'FrozenOpenCLIPTextVisualEmbedder',
    'layer': 'penultimate',
    'pretrained': 'models/open_clip_pytorch_model.bin'
}
auto_encoder: {
    'type': 'AutoencoderKL',
    'ddconfig': {
        'double_z': True, 
        'z_channels': 4,
        'resolution': 256, 
        'in_channels': 3,
        'out_ch': 3, 
        'ch': 128, 
        'ch_mult': [1, 2, 4, 4],
        'num_res_blocks': 2, 
        'attn_resolutions': [], 
        'dropout': 0.0,
        'video_kernel_size': [3, 1, 1]
    },
    'embed_dim': 4,
    'pretrained': 'models/v2-1_512-ema-pruned.ckpt'
}

UNet: {
    'type': 'UNetSD_VideoLCM',
    'config': None,
    'in_dim': 4,
    'dim': 320,
    'y_dim': 1024,
    'context_dim': 1024,
    'out_dim': 4,
    'dim_mult': [1, 2, 4, 4],
    'num_heads': 8,
    'head_dim': 64,
    'num_res_blocks': 2,
    # 'attn_scales': [1 / 1, 1 / 2, 1 / 4],
    'dropout': 0.1,
    'temporal_attention': True,
    'num_tokens': 4,
    'temporal_attn_times': 1,
    'use_checkpoint': True,
    'use_fps_condition': False,
    'use_sim_mask': False
}
video_compositions: ['text']
Diffusion: {
    'type': 'DiffusionDDIM',
    'schedule': 'linear_sd', # cosine
    'schedule_param': {
        'num_timesteps': 1000,
        "init_beta": 0.00085, 
        "last_beta": 0.0120,
        'zero_terminal_snr': True,
    },
    'mean_type': 'v',
    'loss_type': 'mse',
    'var_type': 'fixed_small',
    'rescale_timesteps': False,
    'noise_strength': 0.1
}

Pretrain: {
    'type': pretrain_specific_strategies,
    'fix_weight': False,
    'grad_scale': 0.5,
    'resume_checkpoint': 'models/tft2v_t2v_non_ema_512000.pth', # teacher diffusion model
    'sd_keys_path': 'data/stable_diffusion_image_key_temporal_attention_x1.json',
}

# chunk_size: 4
# decoder_bs: 4
lr: 0.00003

batch_sizes: {
    "1": 32,
    "4": 8,
    "8": 4,
    "16": 16,
    "32": 2
}

noise_strength: 0.1
# classifier-free guidance
p_zero: 0.1
num_steps: 1000000

use_zero_infer: True
viz_interval: 5        # 200
save_ckp_interval: 50   # 500

# Log
log_dir: "workspace/experiments"
log_interval: 1
seed: 8888